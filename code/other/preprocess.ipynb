{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1w3Nr9PqSfDto7U1wiAmJTLnd_Se3nUZy","authorship_tag":"ABX9TyMhc+vxwVzLXTotGUg7xGJO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lOmq-RkcH2pK","executionInfo":{"status":"ok","timestamp":1749754955545,"user_tz":-120,"elapsed":263,"user":{"displayName":"Elisabeta Veliaj","userId":"06895822683778078300"}},"outputId":"877bcf7f-962f-4900-ef96-dcd0b8970271"},"outputs":[{"output_type":"stream","name":"stdout","text":["entities=2441  relations=16\n"]}],"source":["# scripts/make_id_maps.py\n","import json, pandas as pd, pathlib\n","\n","triples = pd.read_csv('/content/drive/MyDrive/THESIS/LTN/KG-triples.csv', sep=',',\n","                      names=['h','r','t'], skiprows=1)\n","\n","entities = sorted(set(triples['h']) | set(triples['t']))\n","relations = sorted(triples['r'].unique())\n","\n","pathlib.Path('build').mkdir(exist_ok=True)\n","\n","with open('/content/drive/MyDrive/THESIS/LTN/entities.json', 'w') as f:\n","    json.dump({e:i for i,e in enumerate(entities)}, f, indent=2)\n","\n","with open('/content/drive/MyDrive/THESIS/LTN/relations.json', 'w') as f:\n","    json.dump({r:i for i,r in enumerate(relations)}, f, indent=2)\n","\n","print(f'entities={len(entities)}  relations={len(relations)}')\n"]},{"cell_type":"code","source":["# scripts/split_triples.py\n","\n","import json\n","import random\n","import pandas as pd\n","from pathlib import Path\n","\n","DATA_DIR   = Path('/content/drive/MyDrive/THESIS/LTN')\n","triples = pd.read_csv('/content/drive/MyDrive/THESIS/LTN/KG-triples.csv', sep=',',\n","                      names=['h','r','t'], skiprows=1)\n","\n","# 1) load & shuffle\n","random.seed(42)\n","\n","triples = triples.sample(frac=1, random_state=42).reset_index(drop=True)\n","\n","# 2) compute split sizes\n","n_total  = len(triples)\n","n_train  = int(0.8 * n_total)\n","n_valid  = int(0.1 * n_total)\n","# rest goes to test\n","n_test   = n_total - n_train - n_valid\n","\n","# 3) slice\n","train = triples[:n_train]\n","valid = triples[n_train:n_train + n_valid]\n","test  = triples[n_train + n_valid:]\n","\n","# 4) write out\n","for name, subset in [('train', train),\n","                     ('valid', valid),\n","                     ('test',  test)]:\n","    out_path = DATA_DIR / f'{name}_triples.json'\n","    with open(out_path, 'w') as f:\n","        json.dump(subset.values.tolist(), f, indent=2)\n","    print(f\"{name:5s}: {len(subset)} triples\")\n","\n","print(f\"total: {n_total}, train={n_train}, valid={n_valid}, test={n_test}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s_1qgM-sIEXz","executionInfo":{"status":"ok","timestamp":1749837211757,"user_tz":-120,"elapsed":800,"user":{"displayName":"Elisabeta Veliaj","userId":"06895822683778078300"}},"outputId":"ad8cbe2c-f0ed-43d5-b7ff-1784cb100169"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train: 3280 triples\n","valid: 410 triples\n","test : 411 triples\n","total: 4101, train=3280, valid=410, test=411\n"]}]},{"cell_type":"code","source":["# scripts/compile_empirical_rules.py\n","import json, yaml, numpy as np, csv, pathlib, math\n","\n","# 1) median λ from AMIE\n","pca_vals = []\n","with open('/content/drive/MyDrive/THESIS/LTN/AMIE-rules.csv') as f:\n","    rdr = csv.reader(f, delimiter=',')\n","    next(rdr)  # header\n","    for row in rdr:\n","        pca_vals.append(float(row[3]))\n","lam_median = np.median([-math.log(1-c+1e-2) for c in pca_vals])\n","lam_exp = 2*lam_median\n","\n","# 2) overwrite YAML weights\n","rules = yaml.safe_load(open('/content/drive/MyDrive/THESIS/LTN/empirical_rules.yaml'))\n","for r in rules:\n","    r['weight'] = lam_exp\n","\n","pathlib.Path('build').mkdir(exist_ok=True)\n","json.dump(rules, open('/content/drive/MyDrive/THESIS/LTN/empirical_rules_compiled.json','w'), indent=2)\n","print(f'saved {len(rules)} empirical rules (λ={lam_exp:.2f})')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VJLZUqIDINCh","executionInfo":{"status":"ok","timestamp":1750345945159,"user_tz":-120,"elapsed":1878,"user":{"displayName":"Elisabeta Veliaj","userId":"06895822683778078300"}},"outputId":"35cc81d1-3d82-4807-9e07-b60038c2720f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["saved 9 empirical rules (λ=3.62)\n"]}]},{"cell_type":"code","source":["# build_patient_facts.py\n","#\n","# Reads patient-week records from CSV and writes a JSON-Lines file\n","# with:\n","#   • Numeric literal triples (\"num::${value}\")\n","#   • Categorical HAS_PROP flags\n","#   • Explicit HAS_WEEK and OF_PATIENT relations\n","# so that LTNs can learn from both empirical data and structural semantics.\n","# --------------------------------------------------------------------\n","import pandas as pd\n","import json\n","import math\n","from pathlib import Path\n","\n","# ── Configuration ──────────────────────────────────────────────────────\n","CSV    = \"/content/drive/MyDrive/THESIS/LTN/patient-data.csv\"\n","OUT    = \"/content/drive/MyDrive/THESIS/LTN/patient_facts_with_week.jsonl\"\n","VAL_NS = \"num::\"\n","PREC   = 3\n","\n","# ── Helper Functions ───────────────────────────────────────────────────\n","\n","def lit(val: float) -> str:\n","    \"\"\"Canonical literal node – identical numbers map to one entity.\"\"\"\n","    return f\"{VAL_NS}{round(float(val), PREC)}\"\n","\n","\n","def write_triple(out_f, h: str, r: str, t: str):\n","    out_f.write(json.dumps({\"h\": h, \"r\": r, \"t\": t}) + \"\\n\")\n","\n","\n","def write_num(out_f, h: str, rel: str, value):\n","    \"\"\"Write a numeric triple if value is not NaN\"\"\"\n","    if pd.isna(value):\n","        return\n","    write_triple(out_f, h, rel, lit(value))\n","\n","\n","def write_has_prop(out_f, h: str, prop: str, cond: bool):\n","    \"\"\"Write (h HAS_PROP prop) if cond is True.\"\"\"\n","    if cond:\n","        write_triple(out_f, h, \"HAS_PROP\", prop)\n","\n","# ── Relations & Baselines ──────────────────────────────────────────────\n","NUMERIC_RELATIONS = {\n","    \"glucose_median_after\"  : \"Glucose_median_After meal\",\n","    \"glucose_median_before\" : \"Glucose_median_Before meal\",\n","    \"glucose_min_after\"     : \"Glucose_min_After meal\",\n","    \"glucose_min_before\"    : \"Glucose_min_Before meal\",\n","    \"glucose_max_after\"     : \"Glucose_max_After meal\",\n","    \"glucose_max_before\"    : \"Glucose_max_Before meal\",\n","    \"sleep_mean\"            : \"Sleep_mean\",\n","    \"steps_mean\"            : \"Steps_mean\",\n","    \"intensity_sum\"         : \"Intensity_sum\",\n","    \"weight_mean\"           : \"Weight_mean\",\n","    \"bmi_mean\"              : \"BMI_mean\",\n","    \"med_diet_score\"        : \"MedDietScore\",\n","    \"hyperglycemia\"         : \"Hyperglycemia\",\n","    \"HbA1c\"                 : \"HbA1c\",\n","    \"LDL\"                   : \"LDL\",\n","}\n","\n","# Precompute baselines for week 0:\n","baseline_steps = {}\n","baseline_bmi   = {}\n","\n","df = pd.read_csv(CSV)\n","for _, row in df[df[\"StudyWeek\"] == 0].iterrows():\n","    pid = row[\"ShortId\"]\n","    if not pd.isna(row.get(\"Steps_mean\")):\n","        baseline_steps[pid] = float(row[\"Steps_mean\"])\n","    if not pd.isna(row.get(\"BMI_mean\")):\n","        baseline_bmi[pid]   = float(row[\"BMI_mean\"])\n","\n","# ── Write JSON-Lines ───────────────────────────────────────────────────\n","with open(OUT, \"w\", encoding=\"utf-8\") as out:\n","    for _, row in df.iterrows():\n","        pid      = row[\"ShortId\"]\n","        week_idx = int(row[\"StudyWeek\"])\n","        node     = f\"{pid}_w{week_idx}\"\n","\n","        # 1) Structural semantics ──────────────────────────────────────────\n","        # a) Which week is this?\n","        write_triple(out, node,     \"HAS_WEEK\",    f\"{VAL_NS}{week_idx}\")\n","        # b) Which patient does this belong to?\n","        write_triple(out, node,     \"OF_PATIENT\",  pid)\n","\n","        # 2) Numeric data ────────────────────────────────────────────────\n","        for rel, col in NUMERIC_RELATIONS.items():\n","            if col in row:\n","                write_num(out, node, rel, row[col])\n","\n","        # 3) Categorical flags ───────────────────────────────────────────\n","        # (i) StepsPlus500\n","        if pid in baseline_steps and not pd.isna(row.get(\"Steps_mean\")):\n","            cond = row[\"Steps_mean\"] - baseline_steps[pid] > 500\n","            write_has_prop(out, node, \"StepsPlus500\", cond)\n","\n","        # (ii) GlucoseReg\n","        g_max_a   = row.get(\"Glucose_max_After meal\")\n","        g_max_b   = row.get(\"Glucose_max_Before meal\")\n","        g_min_a   = row.get(\"Glucose_min_After meal\")\n","        g_min_b   = row.get(\"Glucose_min_Before meal\")\n","        if not any(pd.isna(v) for v in (g_max_a, g_max_b, g_min_a, g_min_b)):\n","            cond = (g_max_a <= 180 and g_max_b <= 130 and g_min_a >= 70 and g_min_b >= 70)\n","            write_has_prop(out, node, \"GlucoseReg\", cond)\n","\n","        # (iii) BMIOnTrack\n","        if pid in baseline_bmi and not pd.isna(row.get(\"BMI_mean\")):\n","            target = baseline_bmi[pid] * (1 - 0.05 * (week_idx - 0.5)/12)\n","            cond   = row[\"BMI_mean\"] <= target\n","            write_has_prop(out, node, \"BMIOnTrack\", cond)\n","\n","        # (iv) WeeklyWeigh\n","        write_has_prop(out, node, \"WeeklyWeigh\", not pd.isna(row.get(\"Weight_mean\")))\n","\n","        # (v) SMBGSpread\n","        if not any(pd.isna(v) for v in (g_max_a, g_min_a)):\n","            spread = g_max_a - g_min_a\n","            write_has_prop(out, node, \"SMBGSpread\", spread > 60)\n","\n","        # (vi) SMBGDays3Plus\n","        days = row.get(\"SMBG_days\")\n","        write_has_prop(out, node, \"SMBGDays3Plus\", (not pd.isna(days) and int(days) >= 3))\n","\n","print(f\"Wrote triples to {OUT}\")\n"],"metadata":{"id":"cWd_uh3ce5eY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750514814745,"user_tz":-120,"elapsed":5699,"user":{"displayName":"Elisabeta Veliaj","userId":"06895822683778078300"}},"outputId":"f028ddf5-d50c-4dae-8635-54233d07b409"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Wrote triples to /content/drive/MyDrive/THESIS/LTN/patient_facts_with_week.jsonl\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"n2x2s2Ar0Dcc"},"execution_count":null,"outputs":[]}]}